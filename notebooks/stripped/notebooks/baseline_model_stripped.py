"""
# Bag of words baseline model
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression 
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


post_type = 'post_travel'
data_path = '../data/split/train.csv'
test_data_path = '../data/split/test.csv'
narcism_type = 'adm'


# Read the data
data = pd.read_csv(data_path)
test_data = pd.read_csv(test_data_path)
data.head()


# Vectorize the train data
vectorizer = CountVectorizer()
transformer = TfidfTransformer()
counts = vectorizer.fit_transform(data[post_type].values)
tfidf = transformer.fit_transform(counts)

df_counts = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())
df_counts['narcissism'] = data[narcism_type]
df_counts.head()


# Vectorize the test data
counts = vectorizer.transform(test_data[post_type].values)
tfidf = transformer.transform(counts)

test_counts = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())
test_counts['narcissism'] = test_data[narcism_type]
test_counts.head()


"""
## Compare diffrent models
"""

# Split data into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(df_counts.iloc[:, :-1], df_counts.iloc[:,-1], test_size=0.2, random_state=42)

X_train = df_counts.iloc[:, :-1]
y_train = np.ravel(df_counts[['narcissism']])
X_test = test_counts.iloc[:, :-1]
y_test = np.ravel(test_counts[['narcissism']])


def train_model(model, X_train, y_train):
    model.fit(X_train, y_train)
    return model

def predict(model, X_test):
    return model.predict(X_test)

def evaluate(y_test, y_pred):
    return mean_squared_error(y_test, y_pred)


# Initialize and train the linear regression model
model = LinearRegression()
train_model(model, X_train, y_train)
y_pred = predict(model, X_test)
mse = evaluate(y_test, y_pred)
print(f"Linear Regression MSE: {mse}")


model = MLPRegressor(max_iter=800)
train_model(model, X_train, y_train)
y_pred = predict(model, X_test)
mse = evaluate(y_test, y_pred)
print(f"MLP Regressor MSE: {mse}")


best_model = SVR()
train_model(best_model, X_train, y_train)
y_pred = predict(best_model, X_test)
mse = evaluate(y_test, y_pred)
print(f"SVR MSE: {mse}")


model = RandomForestRegressor()
train_model(model, X_train, y_train)
y_pred = predict(model, X_test)
mse = evaluate(y_test, y_pred)
print(f"Random Forest MSE: {mse}")


model = DecisionTreeRegressor()
train_model(model, X_train, y_train)
y_pred = predict(model, X_test)
mse = evaluate(y_test, y_pred)
print(f"Decision Tree MSE: {mse}")


"""
So far the best effects give SVM and Random Forest, but also the simples solution has rather small MSE - LinearRegression
"""

"""
## Testing on some new data
Generated by chat GPT 3.5 
Prompt: *"Write me a travel post for twitter"*
"""

# Won't work with the current data

new_data = ["Embarking on an exhilarating adventure through the enchanting streets of Kyoto, Japan. ðŸŽŒ From the serene bamboo forests of Arashiyama to the historic temples of Kinkaku-ji and Fushimi Inari Taisha, every corner unveils a tale of tradition and tranquility. #Kyoto #TravelJapan", "Lost in the colorful labyrinth of Marrakech's bustling souks, where the scent of spices fills the air and vibrant textiles dance in the breeze. ðŸ•Œâœ¨ Exploring hidden riads, savoring tagine delights, and getting lost in the magic of Jardin Majorelle. #Marrakech #TravelGoals ðŸŒ´ðŸŒž"]
counts = vectorizer.transform(new_data)
tfidf = transformer.transform(counts)
df_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())
predicted = predict(best_model, df_tfidf)
for doc, category in zip(new_data, predicted):
     print(f'{doc} =>\nnarcism: {category}')


"""
Testing it on data that is not talking about travel (the abortion posts)
"""

counts = vectorizer.transform(data["post_abortion"].values)
tfidf = transformer.transform(counts)

test_counts_ab = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out())
test_counts_ab['narcissism'] = data[narcism_type]
X_test_ab = test_counts_ab.iloc[:, :-1]
y_test_ab = np.ravel(test_counts_ab[['narcissism']])
y_pred_ab = predict(best_model, X_test_ab)
mse = evaluate(y_test_ab, y_pred_ab)
print(f"SVR MSE test_ab: {mse}")


"""
It is interesting that the model rather correctly predicts the 'ADM' narcissism in abortion posts despite being trained on different data. MSE is still less then 1 point (on the scale 1 to 6)
"""

"""
## Hyperparameters tuning
"""





